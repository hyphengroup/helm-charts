apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "airflow-kube-s3.fullname" . }}-config
  labels: {{ include "airflow-kube-s3.labels" . | nindent 4 }}
data:
  pod_template.yaml: |
    apiVersion: v1
    kind: Pod
    metadata:
      name: dummy-name-dont-delete
      namespace: dummy-name-dont-delete
      labels: {{ include "airflow-kube-s3.labels" . | nindent 8 }}
        app.kubernetes.io/component: worker
      annotations:
        {{- with .Values.airflow.podAnnotations }}
        {{- toYaml . | trim | nindent 8 }}
        {{- end }}
        {{- with .Values.airflow.workers.annotations }}
        {{- toYaml . | trim | nindent 8 }}
        {{- end }}
    spec:
      initContainers:
        - name: git-sync
          image: {{ .Values.airflow.dags.git.image.repository}}:{{ .Values.airflow.dags.git.image.tag}}
          imagePullPolicy: {{ .Values.airflow.dags.git.image.pullPolicy | quote }}
          volumeMounts:
            - name: airflow-dags
              mountPath: {{ .Values.airflow.dags.git.root }}
            - name: git-sync-ssh-configmap
              mountPath: /etc/git-secret/known_hosts
              subPath: known_hosts
            - name: git-sync-ssh-secrets
              mountPath: /etc/git-secret/ssh
              subPath: ssh
          env:
            - name: GIT_SYNC_REPO
              value: {{ .Values.airflow.dags.git.url }}
            - name: GIT_SYNC_BRANCH
              value: {{ .Values.airflow.dags.git.branch }}
            - name: GIT_SYNC_DEPTH
              value: {{ .Values.airflow.dags.git.depth | quote }}
            - name: GIT_SYNC_ROOT
              value: {{ .Values.airflow.dags.git.root }}
            - name: GIT_SYNC_DEST
              value: {{ .Values.airflow.dags.git.dest }}
              # the number of seconds between syncs
            - name: GIT_SYNC_WAIT
              value: {{ .Values.airflow.dags.git.wait | quote }}
              # using secret and configmap for cloning over SSH
            - name: GIT_SSH_KEY_FILE
              value: /etc/git-secret/ssh
            - name: GIT_SYNC_SSH
              value: "true"
            - name: GIT_SYNC_ADD_USER
              value: {{ .Values.airflow.dags.git.addUser | quote }}
            - name: GIT_KNOWN_HOSTS
              value:  {{ .Values.airflow.dags.git.ssh.strictHostKeyChecking | quote }}
            - name: GIT_SSH_KNOWN_HOSTS_FILE
              value: /etc/git-secret/known_hosts
            - name: GIT_SYNC_ONE_TIME
              value: "true"
          resources:
            {{- toYaml .Values.airflow.dags.git.resources | trim | nindent 12 }}
      containers:
        - name: base
          args: []
          command: []
          envFrom:
            {{- range .Values.airflow.envFrom.configMaps }}
            - configMapRef:
                name: {{ . }}
            {{- end }}
            - configMapRef:
                name: {{ template "airflow-kube-s3.fullname" . }}-env
            {{- if .Values.airflow.secrets.create }}
            - secretRef:
                name: {{ template "airflow-kube-s3.fullname" . }}-env
            {{- end }}
            {{- range .Values.airflow.envFrom.secrets }}
            - secretRef:
                name: {{ . }}
            {{- end }}
          env:
            - name: AIRFLOW__KUBERNETES__NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: AIRFLOW__CORE__EXECUTOR
              value: LocalExecutor
          resources:
            {{- toYaml .Values.airflow.workers.resources | trim | nindent 12 }}
          volumeMounts:
            - name: airflow-config
              mountPath: /usr/local/airflow/airflow.cfg
              subPath: airflow.cfg
            - name: airflow-config
              mountPath: /usr/local/airflow/pod_template.yaml
              subPath: pod_template.yaml
            - name: airflow-dags
              mountPath: {{ .Values.airflow.dags.git.folderMountPoint }}
          image: {{ .Values.airflow.image.repository }}:{{ .Values.airflow.image.tag }}
          imagePullPolicy: {{ .Values.airflow.image.pullPolicy }}
          ports: []
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
      hostNetwork: false
      restartPolicy: Never
      terminationGracePeriodSeconds: 30
      securityContext:
        runAsUser: 65533
      nodeSelector:
        {}
      affinity:
        {}
      tolerations:
        []
      serviceAccountName: {{ include "airflow-kube-s3.serviceAccountName" . | quote }}
      
      volumes:
        - name: airflow-dags
          emptyDir: {}
        - name: airflow-config
          configMap:
            name: {{ template "airflow-kube-s3.fullname" . }}-config
            defaultMode: 0644 #420
        - name: git-sync-ssh-configmap
          configMap:
            name: {{ .Values.airflow.dags.git.ssh.knownHostsConfigMapRef }}
            defaultMode: 0440
        - name: git-sync-ssh-secrets
          secret:
            secretName: {{ .Values.airflow.dags.git.ssh.keySecretRef }}
            items:
              - key: gitSshKey
                path: ssh
                mode: 0440
  airflow.cfg: |
    [core]
    # The folder where your airflow pipelines live, most likely a
    # subfolder in a code repository
    # This path must be absolute
    dags_folder = {{ compact (list .Values.airflow.dags.git.folderMountPoint .Values.airflow.dags.git.dest .Values.airflow.dags.git.subpath ) | join "/"}}

    # The folder where airflow should store its log files
    # This path must be absolute
    base_log_folder = /usr/local/airflow/logs

    # Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
    # Users must supply an Airflow connection id that provides access to the storage
    # location. If remote_logging is set to true, see UPDATING.md for additional
    # configuration requirements.
    remote_logging = True
    remote_log_conn_id = s3_logging
    remote_base_log_folder = s3://{{ .Values.airflow.s3_logging.bucket_name }}{{ .Values.airflow.s3_logging.bucket_key }}
    encrypt_s3_logs = False

    # Logging level
    logging_level = INFO
    fab_logging_level = WARN

    dag_processor_manager_log_location = /usr/local/airflow/logs/dag_processor_manager/dag_processor_manager.log

    # Default timezone in case supplied date times are naive
    # can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
    default_timezone = Asia/Singapore

    # The executor class that airflow should use. Choices include
    # SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
    executor = KubernetesExecutor

    # The amount of parallelism as a setting to the executor. This defines
    # the max number of task instances that should run simultaneously
    # on this airflow installation
    parallelism = 32

    # The number of task instances allowed to run concurrently by the scheduler
    dag_concurrency = 16

    # Are DAGs paused by default at creation
    dags_are_paused_at_creation = True

    # The maximum number of active DAG runs per DAG
    max_active_runs_per_dag = 16

    # Whether to load the examples that ship with Airflow. It's good to
    # get started, but you probably want to set this to False in a production
    # environment
    load_examples = False

    # Where your Airflow plugins are stored
    plugins_folder = /usr/local/airflow/plugins

    # Secret key to save connection passwords in the db
    fernet_key = $FERNET_KEY

    # Whether to disable pickling dags
    donot_pickle = False

    # How long before timing out a python file import while filling the DagBag
    dagbag_import_timeout = 30

    # How long before timing out a DagFileProcessor, which processes a dag file
    dag_file_processor_timeout = 50

    # Whether to enable pickling for xcom (note that this is insecure and allows for
    # RCE exploits). This will be deprecated in Airflow 2.0 (be forced to False).
    enable_xcom_pickling = False

    # When a task is killed forcefully, this is the amount of time in seconds that
    # it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
    killed_task_cleanup_time = 60

    # Whether to override params with dag_run.conf. If you pass some key-value pairs through `airflow backfill -c` or
    # `airflow trigger_dag -c`, the key-value pairs will override the existing ones in params.
    dag_run_conf_overrides_params = False

    # Worker initialisation check to validate Metadata Database connection
    worker_precheck = False

    # When discovering DAGs, ignore any files that don't contain the strings `DAG` and `airflow`.
    dag_discovery_safe_mode = True

    # The number of retries each task is going to have by default. Can be overridden at dag or task level.
    default_task_retries = 0

    [api]
    # How to authenticate users of the API
    auth_backend = airflow.api.auth.backend.default

    [operators]
    # The default owner assigned to each new operator, unless
    # provided explicitly or passed via `default_args`
    default_owner = airflow
    default_cpus = 1
    default_ram = 512
    default_disk = 512
    default_gpus = 0

    [webserver]
    # The base url of your website as airflow cannot guess what domain or
    # cname you are using. This is used in automated emails that
    # airflow sends to point links to the right web server
    base_url = {{ .Values.airflow.web.baseUrl }}

    # The ip specified when starting the web server
    web_server_host = 0.0.0.0

    # The port on which to run the web server
    web_server_port = {{ .Values.airflow.web.port }}

    # Number of seconds the webserver waits before killing gunicorn master that doesn't respond
    web_server_master_timeout = 120

    # Number of seconds the gunicorn webserver waits before timing out on a worker
    web_server_worker_timeout = 120

    # Number of workers to refresh at a time. When set to 0, worker refresh is
    # disabled. When nonzero, airflow periodically refreshes webserver workers by
    # bringing up new ones and killing old ones.
    worker_refresh_batch_size = 1

    # Number of seconds to wait before refreshing a batch of workers.
    worker_refresh_interval = 30

    # Secret key used to run your flask app
    secret_key = temporary_key

    # Number of workers to run the Gunicorn web server
    workers = 4

    # The worker class gunicorn should use. Choices include
    # sync (default), eventlet, gevent
    worker_class = sync

    # Log files for the gunicorn webserver. '-' means log to stderr.
    access_logfile = -
    error_logfile = -

    # Expose the configuration file in the web server
    # This is only applicable for the flask-admin based web UI (non FAB-based).
    # In the FAB-based web UI with RBAC feature,
    # access to configuration is controlled by role permissions.
    expose_config = True

    # Default DAG view.  Valid values are:
    # tree, graph, duration, gantt, landing_times
    dag_default_view = tree

    # Default DAG orientation. Valid values are:
    # LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
    dag_orientation = LR

    # The amount of time (in secs) webserver will wait for initial handshake
    # while fetching logs from other worker machine
    log_fetch_timeout_sec = 5

    # By default, the webserver shows paused DAGs. Flip this to hide paused
    # DAGs by default
    hide_paused_dags_by_default = False

    # Consistent page size across all listing views in the UI
    page_size = 100

    # Use FAB-based webserver with RBAC feature
    rbac = {{ .Values.airflow.rbac.enabled }}

    # Define the color of navigation bar
    navbar_color = #007A87

    # Default dagrun to show in UI
    default_dag_run_display_number = 25

    # Default setting for wrap toggle on DAG code and TI log views.
    default_wrap = False

    # Update FAB permissions and sync security manager roles
    # on webserver startup
    update_fab_perms = True

    [email]
    email_backend = airflow.utils.email.send_email_smtp

    [smtp]
    # If you want airflow to send emails on retries, failure, and you want to use
    # the airflow.utils.email.send_email_smtp function, you have to configure an
    # smtp server here
    smtp_host = localhost
    smtp_starttls = True
    smtp_ssl = False
    # Uncomment and set the user/pass settings if you want to use SMTP AUTH
    # smtp_user = airflow
    # smtp_password = airflow
    smtp_port = 25
    smtp_mail_from = airflow@example.com

    [scheduler]
    child_process_log_directory = /usr/local/airflow/logs/scheduler

    # How often (in seconds) to scan the DAGs directory for new files. Default was 5 minutes.
    dag_dir_list_interval = {{ .Values.airflow.dags.git.dirListInterval }}

    [admin]
    # UI to hide sensitive variable fields when set to True
    hide_sensitive_variable_fields = True

    [kubernetes]
    # Path to the YAML pod file. If set, all other kubernetes-related fields are ignored.
    # (This feature is experimental)
    pod_template_file = /usr/local/airflow/pod_template.yaml

    # Airflow potentially overwrites the base image: from the airflow.cfg or per dag using pod_override in kubeExecutor
    worker_container_repository = {{ .Values.airflow.image.repository }}
    worker_container_tag = {{ .Values.airflow.image.tag }}
    worker_container_image_pull_policy = {{ .Values.airflow.image.pullPolicy }}
    
    # If True, all worker pods will be deleted upon termination
    delete_worker_pods = True

    # If False (and delete_worker_pods is True),
    # failed worker pods will not be deleted so users can investigate them.
    delete_worker_pods_on_failure = {{ .Values.airflow.workers.deleteOnFailure }}

    # For docker image already contains DAGs, this is set to `True`, and the worker will search for dags in dags_folder,
    # otherwise use git sync or dags volume claim to mount DAGs
    dags_in_image = False

    # Number of Kubernetes Worker Pod creation calls per scheduler loop
    worker_pods_creation_batch_size = 1

    # Use the service account kubernetes gives to pods to connect to kubernetes cluster.
    # It's intended for clients that expect to be running inside a pod running on kubernetes.
    # It will raise an exception if called from a process not running in a kubernetes environment.
    in_cluster = True

    # When running with in_cluster=False change the default cluster_context or config_file
    # options to Kubernetes client. Leave blank these to use default behaviour like `kubectl` has.
    # cluster_context =
    # config_file =

    # **kwargs parameters to pass while calling a kubernetes client core_v1_api methods from Kubernetes Executor
    # provided as a single line formatted JSON dictionary string.
    # List of supported params in **kwargs are similar for all core_v1_apis, hence a single config variable for all apis
    # See:
    #   https://raw.githubusercontent.com/kubernetes-client/python/master/kubernetes/client/apis/core_v1_api.py
    # Note that if no _request_timeout is specified, the kubernetes client will wait indefinitely for kubernetes
    # api responses, which will cause the scheduler to hang. The timeout is specified as [connect timeout, read timeout]
    kube_client_request_args = {"_request_timeout" : [60,60] }

    # Optional keyword arguments to pass to the ``delete_namespaced_pod`` kubernetes client
    # ``core_v1_api`` method when using the Kubernetes Executor.
    # This should be an object and can contain any of the options listed in the ``v1DeleteOptions``
    # class defined here:
    # https://github.com/kubernetes-client/python/blob/41f11a09995efcd0142e25946adc7591431bfb2f/kubernetes/client/models/v1_delete_options.py#L19
    # Example: delete_option_kwargs = {"grace_period_seconds": 10}
    delete_option_kwargs =
